{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de CorrigeTD1ProgGPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hikachhu/ProgGPU/blob/main/Copie_de_CorrigeTD1ProgGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executez cette cellule pour installer la bonne version de CUDA"
      ],
      "metadata": {
        "id": "LP9Tl8IQxQAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-9.2"
      ],
      "metadata": {
        "id": "SC-ZfHdkMwiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjov-qc1v32O",
        "outputId": "72e555f0-56b4-4a0b-8e29-337f3c494941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Wed_Apr_11_23:16:29_CDT_2018\n",
            "Cuda compilation tools, release 9.2, V9.2.88\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNTbL4fywF2i",
        "outputId": "d526bf3c-d54f-4328-9483-4ec5b77fd62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-ntu5kdt6\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-ntu5kdt6\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4306 sha256=23e203e6d5ff68ab91d44b9626d0d47f6923453859404aeb6606070a0e1776da\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_qfylp_w/wheels/c5/2b/c0/87008e795a14bbcdfc7c846a00d06981916331eb980b6c8bdf\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktto5DHTwJpf",
        "outputId": "a3187d2d-aa30-43a8-9bd4-ed108ab6cc93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3"
      ],
      "metadata": {
        "id": "vdZBR7NSxMLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "__global__ void kernel(void){\n",
        "    printf(\"Hello world from block %d, thread %d\\n\", blockIdx.x, threadIdx.x);\n",
        "}\n",
        "\n",
        "int main(void){\n",
        "    kernel<<<10,10>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "KssMuanhxSgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d16187b-3684-46bb-cec5-6b67873c20d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world from block 8, thread 0\n",
            "Hello world from block 8, thread 1\n",
            "Hello world from block 8, thread 2\n",
            "Hello world from block 8, thread 3\n",
            "Hello world from block 8, thread 4\n",
            "Hello world from block 8, thread 5\n",
            "Hello world from block 8, thread 6\n",
            "Hello world from block 8, thread 7\n",
            "Hello world from block 8, thread 8\n",
            "Hello world from block 8, thread 9\n",
            "Hello world from block 6, thread 0\n",
            "Hello world from block 6, thread 1\n",
            "Hello world from block 6, thread 2\n",
            "Hello world from block 6, thread 3\n",
            "Hello world from block 6, thread 4\n",
            "Hello world from block 6, thread 5\n",
            "Hello world from block 6, thread 6\n",
            "Hello world from block 6, thread 7\n",
            "Hello world from block 6, thread 8\n",
            "Hello world from block 6, thread 9\n",
            "Hello world from block 9, thread 0\n",
            "Hello world from block 9, thread 1\n",
            "Hello world from block 9, thread 2\n",
            "Hello world from block 9, thread 3\n",
            "Hello world from block 9, thread 4\n",
            "Hello world from block 9, thread 5\n",
            "Hello world from block 9, thread 6\n",
            "Hello world from block 9, thread 7\n",
            "Hello world from block 9, thread 8\n",
            "Hello world from block 9, thread 9\n",
            "Hello world from block 3, thread 0\n",
            "Hello world from block 3, thread 1\n",
            "Hello world from block 3, thread 2\n",
            "Hello world from block 3, thread 3\n",
            "Hello world from block 3, thread 4\n",
            "Hello world from block 3, thread 5\n",
            "Hello world from block 3, thread 6\n",
            "Hello world from block 3, thread 7\n",
            "Hello world from block 3, thread 8\n",
            "Hello world from block 3, thread 9\n",
            "Hello world from block 7, thread 0\n",
            "Hello world from block 7, thread 1\n",
            "Hello world from block 7, thread 2\n",
            "Hello world from block 7, thread 3\n",
            "Hello world from block 7, thread 4\n",
            "Hello world from block 7, thread 5\n",
            "Hello world from block 7, thread 6\n",
            "Hello world from block 7, thread 7\n",
            "Hello world from block 7, thread 8\n",
            "Hello world from block 7, thread 9\n",
            "Hello world from block 2, thread 0\n",
            "Hello world from block 2, thread 1\n",
            "Hello world from block 2, thread 2\n",
            "Hello world from block 2, thread 3\n",
            "Hello world from block 2, thread 4\n",
            "Hello world from block 2, thread 5\n",
            "Hello world from block 2, thread 6\n",
            "Hello world from block 2, thread 7\n",
            "Hello world from block 2, thread 8\n",
            "Hello world from block 2, thread 9\n",
            "Hello world from block 5, thread 0\n",
            "Hello world from block 5, thread 1\n",
            "Hello world from block 5, thread 2\n",
            "Hello world from block 5, thread 3\n",
            "Hello world from block 5, thread 4\n",
            "Hello world from block 5, thread 5\n",
            "Hello world from block 5, thread 6\n",
            "Hello world from block 5, thread 7\n",
            "Hello world from block 5, thread 8\n",
            "Hello world from block 5, thread 9\n",
            "Hello world from block 4, thread 0\n",
            "Hello world from block 4, thread 1\n",
            "Hello world from block 4, thread 2\n",
            "Hello world from block 4, thread 3\n",
            "Hello world from block 4, thread 4\n",
            "Hello world from block 4, thread 5\n",
            "Hello world from block 4, thread 6\n",
            "Hello world from block 4, thread 7\n",
            "Hello world from block 4, thread 8\n",
            "Hello world from block 4, thread 9\n",
            "Hello world from block 0, thread 0\n",
            "Hello world from block 0, thread 1\n",
            "Hello world from block 0, thread 2\n",
            "Hello world from block 0, thread 3\n",
            "Hello world from block 0, thread 4\n",
            "Hello world from block 0, thread 5\n",
            "Hello world from block 0, thread 6\n",
            "Hello world from block 0, thread 7\n",
            "Hello world from block 0, thread 8\n",
            "Hello world from block 0, thread 9\n",
            "Hello world from block 1, thread 0\n",
            "Hello world from block 1, thread 1\n",
            "Hello world from block 1, thread 2\n",
            "Hello world from block 1, thread 3\n",
            "Hello world from block 1, thread 4\n",
            "Hello world from block 1, thread 5\n",
            "Hello world from block 1, thread 6\n",
            "Hello world from block 1, thread 7\n",
            "Hello world from block 1, thread 8\n",
            "Hello world from block 1, thread 9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4"
      ],
      "metadata": {
        "id": "4TUi2GvA35r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "#define N 512\n",
        "#define Niter 1000\n",
        "\n",
        "void addCPU(int *a, int *b, int *c) {\n",
        "    for (int i = 0; i < N*N ; i++) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void addKernel1D (int *a, int *b, int *c) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    while (tid < N*N) {\n",
        "        c[tid] = a[tid] + b[tid];\n",
        "        tid += blockDim.x * gridDim.x;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    int A[N*N], B[N*N], C[N*N], D[N*N];\n",
        "    int *d_A, *d_B, *d_C;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaMalloc(&d_A, N*N*sizeof(int));\n",
        "    cudaMalloc(&d_B, N*N*sizeof(int));\n",
        "    cudaMalloc(&d_C, N*N*sizeof(int));\n",
        "\n",
        "    for (int i = 0 ; i < N*N ; i++) {\n",
        "            A[i] = i;\n",
        "            B[i] = i+1;\n",
        "    }\n",
        "    cudaMemcpy(d_A, A, N*N*sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B, N*N*sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    auto t_start = std::chrono::high_resolution_clock::now();\n",
        "    for (int i = 0; i <  Niter ; i++) // We do Niter to have an averaged execution time\n",
        "      addCPU(A, B, C);\n",
        "    auto t_end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    cudaDeviceProp properties;\n",
        "    cudaGetDeviceProperties(&properties, 0);\n",
        "    int nThreads = properties.maxThreadsPerBlock; // Get maximum number of threads per block\n",
        "    int nBlocks = (N*N + nThreads - 1) / nThreads; // Get right number of blocks to cover our problem\n",
        "    addKernel1D<<<nBlocks,nThreads>>>(d_A, d_B, d_C); // WARM-UP\n",
        "    cudaEventRecord(start);\n",
        "    for (int i = 0; i <  Niter ; i++) // We do Niter to have an averaged execution time\n",
        "      addKernel1D<<<nBlocks,nThreads>>>(d_A, d_B, d_C);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    float ms;\n",
        "    cudaEventElapsedTime(&ms, start, stop);\n",
        "\n",
        "    cudaMemcpy(D, d_C, N*N*sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(B, d_B, N*N*sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    int diff = 0;\n",
        "    for (int i = 0 ; i < N ; i++) {\n",
        "        if(D[i] != C[i]) diff = D[i] - C[i];\n",
        "    }\n",
        "    if(diff != 0) {\n",
        "        printf(\"Wrong computation : diff = %d\", diff);\n",
        "        return 0;\n",
        "    }\n",
        "    printf(\"CPU execution time = %f ms\\n\", std::chrono::duration<double, std::milli>(t_end-t_start).count()/Niter);\n",
        "    printf(\"GPU execution time = %f ms\\n\", ms/Niter);\n",
        "    printf(\"%d %d %d\\n\",A[2],B[2],C[2]);\n",
        "  cudaFree(d_A);\n",
        "  cudaFree(d_B);\n",
        "  cudaFree(d_C);\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYT9qRDa37rS",
        "outputId": "4d3f5fd4-1034-4eeb-c35f-a672a75dd263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU execution time = 0.876779 ms\n",
            "GPU execution time = 0.027474 ms\n",
            "2 3 5\n",
            "\n"
          ]
        }
      ]
    }
  ]
}